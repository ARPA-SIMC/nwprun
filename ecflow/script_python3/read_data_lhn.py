import os, re, sys, csv, argparse, datetime as dt

# -------------------------------------------------------------------------------------
# DESCRIPTION - 17/08/2022
# -------------------------------------------------------------------------------------
# The script reads YULHN files generated by COSMO and stored in directories named as 
# analysis date and time (in the format YYYYMMDDHH) contained in --folder. The CSV file 
# "diagnostic_LHN.csv" is generated reporting, for each analysis date and time:
#   - the percentage of available SRI files for the assimilation cycle;
#   - average, minimum and maximum SRI over the radar domain during the assimilation
#     cycle.
# Finally, the file 'last_date_lhn.txt' is generated, in which last date that should be 
# plotted by 'plot_data_lhn.py' is saved.
#
# If the script is launched for the first time, that is "diagnostic_LHN.csv" file does
# not exist, all YULHN iles in --folder are read. Otherwise, the script retrieves from
# the CSV file the date and time of the last YULHN file analyzed, and starts to read 
# from the following one.


# -------------------------------------------------------------------------------------
# INPUT VARIABLES AND FUNCTIONS 
# -------------------------------------------------------------------------------------
# Command line input
def command_line():
    parser = argparse.ArgumentParser(description = 'LHN DIAGNOSTIC - READ YULHN FILES.\
                        The script read YULHN files and save some information for each\
                        analysis date and time, which can be plot with                \
                        plot_data_lhn.py')
    parser.add_argument('--folder',   default = ".",   type = str,
                        help = 'Folder in which YULHN files are stored, contained in  \
                                in subdirectories named as analysis date and time in  \
                                the format YYYYMMDDHH defalut: current directory)')
    parser.add_argument('--lencyc',   default = 1,     type = int,
                        help = 'Length of assimilation cycles in hours')
    parser.add_argument('--sri_freq', default = 10,    type = int,
                        help = 'Frequemcy of SRI files employed for LHN in minutes.')
    return parser.parse_args()


# Save on csv file. Parameters: 
# fname  = file name
# header = header of entries
# body   = list of entries
def save_on_csv(fname, header, body):
    # Create header if a new file is created
    if not os.path.isfile(fname):
        with open(fname, mode='w') as csvfile:
            file_writer = csv.writer(csvfile, delimiter=',')
            file_writer.writerow(header)

    # Save data on file
    with open(fname, mode='a+') as csvfile:
        file_writer = csv.writer(csvfile, delimiter=',')
        file_writer.writerow(body)

    return None
 

# Get string with date, minimum, maximum and average 
def get_lhn_string(fname):
    # Save lines containing these data
    saved_lines = []
    with open(fname, 'rt') as f:
        data = f.readlines()
    for line in data:
        if line.__contains__('Stat. of obs    at'):
            saved_lines.append(line)
  
    return saved_lines

 
# Get floats from string
def get_lhn_data(line):
    fl_list = []
    for t in line.split():
        try:
            fl_list.append(float(t))
        except ValueError:
            pass
    
    return fl_list


# -------------------------------------------------------------------------------------
# INITIALIZE SOME VARIABLES AND DETERMINE WHICH YULHN FILES HAVE TO BE READ
# -------------------------------------------------------------------------------------
# Read variables from commad line
args   = command_line()
folder, lencyc, sri_freq = args.folder, args.lencyc, args.sri_freq

# Define header for the csv file
header = ['Date', 'Time', 'Percentage of available obs.', 'Average', 'Minimum',
          'Maximum']

# Define output file name
fname_lhn = "diagnostic_LHN.csv"

# Some prints
print("\n------------------------------------------------------------")
print("LHN DIAGNOSTICS")
print("------------------------------------------------------------")

# List directories in "folder" in which YULHN files are contained (analysis date and 
# time in the format "YYYYMMDDHH")
dir_list      = sorted(next(os.walk(folder))[1])
enda_cycle_dt = []
for dir_sing in dir_list:
    if dir_sing.startswith("20"):
        enda_cycle_dt.append(dir_sing)

# Compute number of expected files assimilated via LHN in each assimilation cycle
exp_files = int(lencyc*60/sri_freq) + 1 

# Detremine from which date to start analaysing YULHN files: if the csv file exists,
# save the last date and time in "last_dt"; otherwise set "last_dt" to analyze
# all folders
if os.path.isfile(fname_lhn):
    with open(fname_lhn, "r") as csvfile:
        last_line = list(csvfile.readlines()[-1].split(','))
        last_dt   = dt.datetime.strptime("%s %s" %(last_line[0], last_line[1]),
                                               '%Y%m%d %H%M')
        print("Last date in file '%s':%s \n" %(fname_lhn, last_dt))
else:
    last_dt = dt.datetime.strptime(enda_cycle_dt[0], '%Y%m%d%H') -         \
              dt.timedelta(hours=1)
    print("File '%s' does not exixt. All folders in '%s' will be analyzed\n" \
          %(fname_lhn, folder))


# -------------------------------------------------------------------------------------
# READ YULHN FILES FOR EACH ANALYSIS DATE AND TIME
# -------------------------------------------------------------------------------------
# Loop over folders containing YULHN files 
for fold in enda_cycle_dt:
    # Transform folder name in datetime variable and skip folders which have already 
    # been analyzed
    ana_dt = dt.datetime.strptime(fold, '%Y%m%d%H')
    if ana_dt <= last_dt: continue

    # Check if YULHN file exists, otherwise skip to next folder
    print("Analyzing YULHN file for %s:" %fold)
    fname  = "%s/%s/YULHN" %(folder, fold)
    try:
        f = open(fname)
        f.close()
    except:
        print('  - YULHN file does not exist\n')
        continue

    # Retrieve date and time of analysis and of cycle start
    dt_start_cycle = ana_dt - dt.timedelta(hours=lencyc)
    ana_date       = dt.datetime.strftime(ana_dt, '%Y%m%d')
    ana_time       = dt.datetime.strftime(ana_dt, '%H%M')

    # Get lines starting with "Stat. of obs    at", that is lines containing date
    # minimum, maximum and average of SRI files. If no line is retrieved, skip to 
    # nect folder
    file_strings  = get_lhn_string(fname)
    if file_strings == []:
        print("  - YULHN file for %s does not contain any string " %fold, 
              "'Stat. of obs    at' and, therefore, \n    this analysis is "
              "discarded.\n")
        continue

    # For each of these lines, get data of SRI file (date, min, max and average)
    num_val_files = 0
    SRI_min_cyc, SRI_max_cyc, SRI_sum_cyc, SRI_ave_cyc = 0, 0, 0, 0
    for line in file_strings:
        # Get a list of floats from "line"
        all_data = get_lhn_data(line)

        # Get date of the SRI file (in datetime format)
        date     = dt.datetime.strptime(str(int(all_data[0])), '%Y%m%d%H%M')

        # Consider valid files, inside the assimilation cycle
        if date >= dt_start_cycle and date <= ana_dt:
            if len(all_data) > 1:
                num_val_files = num_val_files + 1

            # Update minimum, maximum and sum mean values of SRI for the current
            # assimilation cycle
            if len(all_data) > 1:
                SRI_min_cyc = round(min(i for i in [SRI_min_cyc, all_data[1]] if i \
                                    is not None), 1)
                SRI_max_cyc = round(max(i for i in [SRI_max_cyc, all_data[2]] if i \
                                    is not None), 1)
                SRI_sum_cyc = sum(i for i in [SRI_sum_cyc, all_data[3]] if i       \
                                    is not None)


    # ---------------------------------------------------------------------------------
    # COMPUTE AVERAGE, PERCENTAGE AND SAVE
    # ---------------------------------------------------------------------------------
    # Compute average
    if num_val_files > 0:
        SRI_ave_cyc = round(SRI_sum_cyc/num_val_files, 3)
    
    # Compute percentage of available files
    perc_files = round(num_val_files/exp_files * 100)

    # Some print
    print("  - Percentage of available SRI files: %s " %("{:>6}".format(perc_files)))
    print("  - Average SRI:                       %s"   %("{:>6}".format(SRI_ave_cyc)))
    print("  - Minimum SRI:                       %s"   %("{:>6}".format(SRI_min_cyc)))
    print("  - Maximum SRI:                       %s\n" %("{:>6}".format(SRI_max_cyc)))

    # Save on csv file
    save_on_csv(fname_lhn, header,
                [ana_date, ana_time, perc_files, SRI_ave_cyc, SRI_min_cyc,
                 SRI_max_cyc])

# Save last date that should be plotted
dt_last = enda_cycle_dt[-1]
with open('last_date_lhn.txt', mode='w') as datefile:
    datefile.write(dt_last)

