import argparse, sys, csv, os
import numpy as np, datetime as dt
from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/

# -------------------------------------------------------------------------------------
# DESCRIPTION - 09/12/2020
# -------------------------------------------------------------------------------------
# The script reads ekf files generated by KENDA and stored in directories named as 
# analysis date and time (in the format YYYYMMDDHH) contained in --folder. Employing
# the information contained in each ekf file, for each report type specified by 
# --report, the following files are generated:
#   - REP.csv: reports, for each analysis date and time, the total number of 
#       observations and the number of them which are set active, passive or rejected
#       ('REP' is replaced by the report name);
#   - REP_passive_checks.csv: reports, for each analysis date and time, the count of
#       each value of the 'checks' ekf variable for all the passive observations ('REP'
#       is replaced by the report name);
#   - REP_rejected_checks.csv: as 'REP_passive_checks.csv' but for rejected 
#       observations;
#   - REP_varX.csv: same as 'REP.csv' but considering only the observation variable 'X'
#       which value is set according to 'varno' ekf variable;
#   - REP_varX_passive_checks.csv: same as 'REP_passive_checks.csv' but considering 
#       only the observation variable 'X';
#   - REP_varX_rejected_checks.csv: same as 'REP_rejected_checks.csv' but considering
#       only the observation variable 'X';
# For generating these files, only observation variables defined by 'relevant_var' 
# are considered.
# Finally, the file 'last_date.txt' is generated, in which last date that should be 
# plotted by 'plot_ekf.py' is saved.
#
# If the script is launched for the first time for one or more reports, that is the
# 'REP.csv' file is not present, all ekf files in --folder for that report are read.
# Otherwise, the script retrieves from 'REP.csv' the date and time of the last ekf
# file analyzed, and starts to read from the following one.


# -------------------------------------------------------------------------------------
# INPUT VARIABLES AND FUNCTIONS 
# -------------------------------------------------------------------------------------
# Observation variables that are considered in this script (according to 'varno' ekf 
# variable)
relevant_var = [1, 2, 3, 4, 7, 8, 9, 10, 11, 29, 39, 40, 41, 42, 45, 56,  57, 58, 59, 
                110, 118, 192, 193, 241, 251]


# Command line input
def command_line():
    parser = argparse.ArgumentParser(description = 'KENDA DIAGNOSTIC - READ EKF FILES.\
                        The script read ekf files and save some information for each  \
                        observation report, which can be plot with plot_ekf.py ')
    parser.add_argument('--folder',   default = ".",   type = str,
                        help = 'Folder in which ekf files are stored, contained in    \
                                in subdirectories named as analysis date and time in  \
                                the format YYYYMMDDHH defalut: current directory)')
    parser.add_argument('--report',  default = ["AIREP", "SYNOP", "TEMP"],
                        nargs='+',   type = str,
                        help = 'Blank space separated list of observation reports to  \
                                analyze (default: AIREP SYNOP TEMP)')
    return parser.parse_args()


# Save on csv file. Parameters: 
# fname  = file name
# header = header of entries
# body   = list of entries
def save_on_csv(fname, header, body):
    # Create header if a new file is created
    if not os.path.isfile(fname):
        with open(fname, mode='w') as csvfile:
            file_writer = csv.writer(csvfile, delimiter=',')
            file_writer.writerow(header)
    
    # Save data on file
    with open(fname, mode='a+') as csvfile:
        file_writer = csv.writer(csvfile, delimiter=',')
        file_writer.writerow(body)

    return None


# Read data from csv file. Parameters: 
#   - fname      = file name
#   - option     = 'dict'   : to store file as a dictionary, considering each row as
#                             if the first element is the key and the second the value
#                             (key: value)
#                  'summary': to acquire files containing 'summary' information
#                  'checks' : to acquire files containing 'checks'  information
#   - check_list = list of headers for checks (that is, headers of the file containing
#                  checks information but without 'Date', 'Time' or other entries not
#                  linked to the count of checks).
def read_from_csv(fname, option, check_list=None):
    # Read the file
    with open(fname, 'r') as csvfile:
        # Read the file as a dictionary for each row (key: value} and
        # return it
        if option == 'dict':
            reader = csv.reader(csvfile)
            mydict = {int(rows[0]):rows[1] for rows in reader}
            return mydict
        # Read the file as a dictionary for each column {header: value)
        # to be decomposed, after, into arrays
        else:
            reader = csv.DictReader(csvfile)
            data = {}
            for row in reader:
                for header, value in row.items():
                    try:
                        data[header].append(value)
                    except KeyError:
                        data[header] = [value]

    # Store date and time in a list of datetime objects and other data 
    # according to 'option'
    date     = np.array(data['Date'])
    time     = np.array(data['Time'])
    datetime = np.array([dt.datetime.strptime("%s %s" %(date[i], time[i]),
                        '%Y%m%d %H%M') for i in range(len(date))])
    if option == 'summary':
        tot_obs = np.array(data['Total'],    dtype=int)
        act_obs = np.array(data['Active'],   dtype=int)
        pas_obs = np.array(data['Passive'],  dtype=int)
        rej_obs = np.array(data['Rejected'], dtype=int)
        return datetime, tot_obs, act_obs, pas_obs, rej_obs
    elif option == 'checks':
        for cn in check_list:
            chk_cn  = np.array(data[cn], dtype=int)
            if cn == check_list[0]:
                checks = chk_cn
            else:
                checks = np.c_[checks, chk_cn]
        return datetime, checks


# -------------------------------------------------------------------------------------
# INITIALIZE SOME VARIABLES AND DETERMINE WHICH EKF FILES HAVE TO BE READ
# -------------------------------------------------------------------------------------
# Read variables from commad line
args            = command_line()
folder, report = args.folder, args.report
if len(report) == 0:
    sys.exit("ERROR! You must specify a valid string for --report")

# Import dictionaries for checks and observations
chk_dict = read_from_csv('dictionary_checks.csv',    'dict')
var_dict = read_from_csv('dictionary_variables.csv', 'dict')

# Define header for 'summary' and 'checks' files
head_sum = ['Date', 'Time', 'Total', 'Active', 'Passive', 'Rejected']
head_chk = ['Date', 'Time']
chk_list = []
for i in range(len(chk_dict)):
    chk_list.append('c%s' %i)
head_chk = head_chk + chk_list

# List directories in "folder" in which ekf files are contained (analysis date and 
# time in the forma t"YYYYMMDDHH")
dir_list      = sorted(next(os.walk(folder))[1])
enda_cycle_dt = []
for dir_sing in dir_list:
    if dir_sing.startswith("20"):
        enda_cycle_dt.append(dir_sing)

# Loop over reports
for rep in report:
    print("\n------------------------------------------------------------")
    print("%s" %rep)
    print("------------------------------------------------------------")

    # Detremine from which date to start analaysing ekf files: if the csv file exists,
    # save the last date and time in "last_dt"; otherwise set "last_dt" to analyze
    # all folders
    fname_sum = "%s.csv" %rep
    if os.path.isfile(fname_sum):
        with open(fname_sum, "r") as csvfile:
            last_line = list(csvfile.readlines()[-1].split(','))
            last_dt   = dt.datetime.strptime("%s %s" %(last_line[0], last_line[1]), 
                                                   '%Y%m%d %H%M') 
            print("Last date in file '%s': " %fname_sum, last_dt)
    else: 
        last_dt = dt.datetime.strptime(enda_cycle_dt[0], '%Y%m%d%H') -         \
                  dt.timedelta(hours=1)
        print("File '%s' does not exixt. All folders in '%s' will be analyzed" \
              %(fname_sum, folder))


    # ---------------------------------------------------------------------------------
    # READ EKF FILES FOR EACH ANALYSIS DATE AND TIME
    # ---------------------------------------------------------------------------------
    # Loop over folders containing ekf files
    for fold in enda_cycle_dt:
        # Transform folder name in datetime variable and skip folders which have 
        # already been analyzed
        fold_dt = dt.datetime.strptime(fold, '%Y%m%d%H')
        if fold_dt <= last_dt: continue

        # Open ekf file in the specific folder, if it exists
        print("\nLooking for ekf file in: %s" %fold)
        ncname = "%s/%s/ekf%s.nc" %(folder, fold, rep)
        try:
            nc_fid = Dataset(ncname, 'r') 
        except:
            print("  File does not exixst")
            continue
    
        # Get analysis date and time
        ana_date =         nc_fid.getncattr('verification_ref_date')
        ana_time = "%04d" %nc_fid.getncattr('verification_ref_time')
        print("Analysis date and time: ", ana_date, ana_time)
        
        # Get allocated and actual number of reports and observations
        d_hdr  = len(nc_fid.dimensions['d_hdr']) 
        d_body = len(nc_fid.dimensions['d_body'])
        n_hdr  = nc_fid.getncattr('n_hdr')
        n_body = nc_fid.getncattr('n_body')
        
        # Variables with "d_hdr"  dimension
        ibody = nc_fid.variables['i_body'][:]
        lbody = nc_fid.variables['l_body'][:]
        
        # Variables with "d_body" dimension
        varno = nc_fid.variables['varno'][:]
        state = nc_fid.variables['state'][:]
        check = nc_fid.variables['check'][:]
        
        # Total active, passive and rejected observations for the whole file
        act = sum(i <= 1            for i in state)
        pas = sum(i >= 3 and i <= 5 for i in state)
        rej = sum(i >= 7            for i in state)

        # Initialize variables to save information about relevant observations
        tot_rel, act_rel, pas_rel, rej_rel = 0, 0, 0, 0
        check_pas_rel = np.zeros(len(chk_dict), dtype=int)
        check_rej_rel = np.zeros(len(chk_dict), dtype=int)

        # Loop over specific observation variables
        print("Relevant observations                             ",
              "total |  active | passive | rejected")
        unique_varno = list(set(varno))
        excl_var = [] 
        for uvar in unique_varno:
            # Exclude not relevant variables
            if uvar not in relevant_var: 
                excl_var.append(uvar)
                continue

            # Total active, passive and rejected observations for the specific
            # observation variable
            ind_var   = np.where(varno == uvar)
            count_var = np.size(ind_var)
            state_var = state[ind_var]
            check_var = check[ind_var]
            act_var   = sum(i <= 1            for i in state_var)
            pas_var   = sum(i >= 3 and i <= 5 for i in state_var)
            rej_var   = sum(i >= 7            for i in state_var)
            print("- %s: %s | %s | %s | %s" %("{:<45}".format(var_dict[uvar]), 
                    "{:>7}".format(count_var),"{:>7}".format(act_var),
                    "{:>7}".format(pas_var),  "{:>7}".format(rej_var)))
            
            # Save on file: specific observation variable summary data
            fname_obs = "%s_var%s.csv" %(rep, uvar)
            list_save = [ana_date, ana_time, count_var, act_var, pas_var,  rej_var]
            save_on_csv(fname_obs, head_sum, list_save)

            # Store type and number of check for passive observations
            check_arr_pas = np.zeros(len(chk_dict), dtype=int)
            if pas_var > 0:
                print("   + Reason for being set passive:")
                ind_var_pas   = np.where((state_var >= 3) & (state_var <= 5))
                check_var_pas = check_var[ind_var_pas]           
                unique_chk    = list(set(check_var_pas))
                for uchk in unique_chk:
                    if uchk == 32: continue
                    count_chk = sum(i == uchk for i in check_var_pas)
                    check_arr_pas[uchk] = count_chk
                    print("     - %s: %s" %("{:<40}".format(chk_dict[uchk]), 
                                            "{:>27}".format(count_chk)))

                # Control that sum of checks is equal to the number of passive obs.
                # Actually, it can happpen that a an observation is passive/rejected
                # with check=32 ("no flag set")
                if not pas_var == np.sum(check_arr_pas):
                    print("\nWARNING! Number of checks (%s) does not match the number\
                           \nof passive observations (%s). This means that some\
                           \nobservations are set passive ''without a reason''\n"
                           %(np.sum(check_arr_pas), pas_var))

            # Save on file: specific observation variable passive checks
            fname_obs_pas = "%s_var%s_passive_checks.csv" %(rep, uvar)
            list_save     = [ana_date, ana_time] + list(check_arr_pas)
            save_on_csv(fname_obs_pas, head_chk, list_save)

            # Store type and number of check for rejected observations
            check_arr_rej = np.zeros(len(chk_dict), dtype=int)
            if rej_var > 0:
                print("   + Reason for being set rejected:")
                ind_var_rej   = np.where(state_var >= 7)
                check_var_rej = check_var[ind_var_rej]
                unique_chk    = list(set(check_var_rej))
                for uchk in unique_chk:
                    if uchk == 32: continue
                    count_chk = sum(i == uchk for i in check_var_rej)
                    check_arr_rej[uchk] = count_chk
                    print("     - %s: %s" %("{:<40}".format(chk_dict[uchk]), 
                                            "{:>37}".format(count_chk)))

                # Control that sum of checks is equal to the number of passive obs.
                # Actually, it can happpen that a an observation is passive/rejected
                # with check=32 ("no flag set")
                if not rej_var == np.sum(check_arr_rej):
                    print("\nWARNING! Number of checks (%s) does not match the number\
                           \nof rejected observations (%s). This means that some\
                           \nobservations are set rejected ''without a reason''\n"
                           %(np.sum(check_arr_rej), rej_var))

            # Save on file: specific observation variable rejected checks
            fname_obs_rej = "%s_var%s_rejected_checks.csv" %(rep, uvar)
            list_save     = [ana_date, ana_time] + list(check_arr_rej)
            save_on_csv(fname_obs_rej, head_chk, list_save)

            # Update information for relevant variables
            tot_rel       += count_var
            act_rel       += act_var
            pas_rel       += pas_var
            rej_rel       += rej_var
            check_pas_rel += check_arr_pas
            check_rej_rel += check_arr_rej

        # Smve on file: report summary data
        print("Report summary          all obs. | relevant obs.")
        print("- Total:    %s | %s" %("{:>20}".format(n_body), "{:>7}".format(tot_rel)))
        print("- Active:   %s | %s" %("{:>20}".format(act),    "{:>7}".format(act_rel)))
        print("- Passive:  %s | %s" %("{:>20}".format(pas),    "{:>7}".format(pas_rel)))
        print("- Rejected: %s | %s" %("{:>20}".format(rej),    "{:>7}".format(rej_rel)))
        if excl_var != []:
            print("- Excluded observaions: ", [var_dict[ev] for ev in excl_var])
        list_save = [ana_date, ana_time, tot_rel, act_rel, pas_rel, rej_rel]
        save_on_csv(fname_sum, head_sum, list_save)

        # Save on file: report passive checks
        fname_pas = "%s_passive_checks.csv" %(rep)
        list_save = [ana_date, ana_time] + list(check_pas_rel)
        save_on_csv(fname_pas, head_chk, list_save)

        # Save on file: report passive checks
        fname_rej = "%s_rejected_checks.csv" %(rep)
        list_save = [ana_date, ana_time] + list(check_rej_rel)
        save_on_csv(fname_rej, head_chk, list_save)


# Save last date that should be plotted (the penultimate analyzed folder is considered
# instead of the last one# because it may not yet contain the ekf files)
dt_last = enda_cycle_dt[-2]
with open('last_date.txt', mode='w') as datefile:
    datefile.write(dt_last)

